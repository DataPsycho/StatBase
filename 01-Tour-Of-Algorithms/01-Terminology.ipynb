{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Terminology and Notations\n",
    "\n",
    "This part we covered the main terminology and mathmatical notation which we will carry out in the whole StatBase project. We will use the iris data set to understand the mathmatical representation of feature sets, dimentation  and outcome variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Iris Data Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Note: The Iris dataset consisting of 150 samples and four features can then be written as a 150Ã— 4 matrix*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation*}\n",
    "\\begin{bmatrix}\n",
    "x_1^{(1)} & x_2^{(1)} & x_3^{(1)} & x_4^{(1)}\\\\\n",
    "x_1^{(2)} & x_2^{(2)} & x_3^{(2)} & x_4^{(2)}\\\\\n",
    "x_1^{(3)} & x_2^{(3)} & x_3^{(3)} & x_4^{(3)}\\\\\n",
    ". & . & . & .\\\\\n",
    ". & . & . & .\\\\\n",
    "x_1^{(150)} & x_2^{(150)} & x_3^{(150)} & x_4^{(150)}\\\\\n",
    "\\end{bmatrix}\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Note: A single sample can be represented as follows. Thus, each row in this feature matrix represents one flower instance and can be written as a four-dimensional row vector.* \n",
    "$x^{(i)} \\in R^{(1\\times4)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation*}\n",
    "    x^{(i)} =\n",
    "    \\begin{bmatrix}\n",
    "    x_1^{(i)} & x_2^{(i)} & x_3^{(i)} & x_4^{(i)}\\\\\n",
    "    \\end{bmatrix}\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Note: Each feature dimension is a 150-dimensional column vector. Similarly, we store the target variables (here, class labels) as a 150-dimensional column vector.*\n",
    "$x^{(i)} \\in R^{(150\\times1)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation*}\n",
    "x_j =\n",
    "\\begin{bmatrix}\n",
    "x_j^{(1)} \\\\ \n",
    "x_j^{(2)} \\\\\n",
    "...\\\\\n",
    "x_j^{(150)}\\\\\n",
    "\\end{bmatrix}\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Similarly, we store the target variables (here, class labels) as a 150-dimensional column vector:*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation*}\n",
    "    y =\n",
    "    \\begin{bmatrix}\n",
    "    y^{(1)} \\\\ \n",
    "    y^{(2)} \\\\\n",
    "    ...\\\\\n",
    "    y^{(150)}\\\\\n",
    "    \\end{bmatrix}\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Roadmap of building ML Application\n",
    "\n",
    "Preprocessing --> Learning --> Evaluation (Go back to Learning or forward to Prediction) --> Prediction\n",
    "\n",
    "- Preprocessing:\n",
    "    - Raw Data > Train Test Split\n",
    "- Learning:\n",
    "    - Train Data > Selected Algorithm > Trained Models (Model Parameters)\n",
    "- Evaluation\n",
    "    - Test Data > Model > Accuracy\n",
    "- Prediction\n",
    "    - New Data > Model > Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parametric versus nonparametric models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Machine learning algorithms can be grouped into parametric and nonparametric models. Using parametric models, we estimate parameters from the training dataset to learn a function that can classify new data points without requiring the original training dataset anymore. Typical examples of parametric models are the perceptron, logistic regression, and the linear SVM.* \n",
    "\n",
    "*In contrast, nonparametric models can't be characterized by a fixed set of parameters, and the number of parameters grows with the training data. Two examples of non-parametric models that we have seen so far are the decision tree classifier/random forest and the kernel SVM.* \n",
    "\n",
    "*KNN belongs to a subcategory of nonparametric models that is described as instance-based learning. Models based on instance-based learning are characterized by memorizing the training dataset, and lazy learning is a special case of instance-based learning that is associated with no (zero) cost during the learning process.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Referances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Python Machine Learning by Sebastian Raschka"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
